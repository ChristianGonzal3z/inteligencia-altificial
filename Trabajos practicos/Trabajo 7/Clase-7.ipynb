{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "---\n",
    "<style>\n",
    "      h1, h2, h3, h4, h5, h6,.imagen {\n",
    "        text-align: center;\n",
    "      }\n",
    " img{width: 75%; height: 75%;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- [Machine Learning](#machine-learning)\n",
    "  - [El Aprendizaje](#el-aprendizaje)\n",
    "  - [El aprendizaje automático](#el-aprendizaje-automático)\n",
    "  - [Clasificación de Machine Learning](#clasificación-de-machine-learning)\n",
    "    - [Aprendizaje supervisado](#aprendizaje-supervisado)\n",
    "    - [Aprendizaje no supervisado](#aprendizaje-no-supervisado)\n",
    "    - [Aprendizaje por refuerzo](#aprendizaje-por-refuerzo)\n",
    "  - [Un Desarrollo Tradicional vs Machine Learning](#un-desarrollo-tradicional-vs-machine-learning)\n",
    "  - [Modelos de Machine Learning](#modelos-de-machine-learning)\n",
    "  - [Pasos para crear un modelo de Machine Learning](#pasos-para-crear-un-modelo-de-machine-learning)\n",
    "    - [Recopilación de datos](#recopilación-de-datos)\n",
    "    - [Explorar y preparar los datos](#explorar-y-preparar-los-datos)\n",
    "    - [Seleccionar y entrenar un modelo](#seleccionar-y-entrenar-un-modelo)\n",
    "    - [Evaluación del rendimiento del modelo](#evaluación-del-rendimiento-del-modelo)\n",
    "    - [Mejora del rendimiento del modelo](#mejora-del-rendimiento-del-modelo)\n",
    "  - [Algoritmos de Machine Learning](#algoritmos-de-machine-learning)\n",
    "    - [Algoritmos Paramétricos y No Paramétricos](#algoritmos-paramétricos-y-no-paramétricos)\n",
    "    - [Sesgo y Varianza](#sesgo-y-varianza)\n",
    "    - [Algoritmo de Regresión Lineal](#algoritmo-de-regresión-lineal)\n",
    "    - [Algoritmo de Regresión Logística](#algoritmo-de-regresión-logística)\n",
    "    - [Algoritmo de Análisis Discriminante Lineal](#algoritmo-de-análisis-discriminante-lineal)\n",
    "    - [Análisis de Componentes Principales](#análisis-de-componentes-principales)\n",
    "    - [Árboles de Clasificación y Regresión](#árboles-de-clasificación-y-regresión)\n",
    "    - [K-Means](#k-means)\n",
    "    - [Algoritmo de los k-Vecinos más Cercanos](#algoritmo-de-los-k-vecinos-más-cercanos)\n",
    "    - [Perceptrón](#perceptrón)\n",
    "    - [Vector de Cuantización de Aprendizaje (Learning Vector Quantization, LVQ)](#vector-de-cuantización-de-aprendizaje-learning-vector-quantization-lvq)\n",
    "    - [Máquinas de Vectores de Soporte](#máquinas-de-vectores-de-soporte)\n",
    "    - [Bagging y Bosques Aleatorios](#bagging-y-bosques-aleatorios)\n",
    "    - [Boosting y AdaBoost](#boosting-y-adaboost)\n",
    "  - [Funciones de Pérdida](#funciones-de-pérdida)\n",
    "  - [Ejemplos de Uso de Machine Learning](#ejemplos-de-uso-de-machine-learning)\n",
    "    - [Sistemas de recomendación y marketing personalizado](#sistemas-de-recomendación-y-marketing-personalizado)\n",
    "    - [Segmentación de mercado](#segmentación-de-mercado)\n",
    "    - [Chatbots](#chatbots)\n",
    "    - [Navegación autónoma](#navegación-autónoma)\n",
    "    - [Mantenimiento predictivo](#mantenimiento-predictivo)\n",
    "    - [Generación de contenido](#generación-de-contenido)\n",
    "    - [Clasificación de datos](#clasificación-de-datos)\n",
    "    - [Predicción y pronóstico](#predicción-y-pronóstico)\n",
    "    - [Reconocimiento de voz y procesamiento del lenguaje natural](#reconocimiento-de-voz-y-procesamiento-del-lenguaje-natural)\n",
    "    - [Visión por computadora](#visión-por-computadora)\n",
    "    - [Medicina y salud](#medicina-y-salud)\n",
    "    - [Automatización industrial](#automatización-industrial)\n",
    "    - [Seguridad cibernética](#seguridad-cibernética)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El Aprendizaje\n",
    "\n",
    "El aprendizaje es el proceso a través del cual se adquieren y desarrollan habilidades, conocimientos, conductas y valores. Este proceso puede ser resultado de la atención, el estudio, la experiencia, la instrucción, el razonamiento o la observación. El aprendizaje es una de las funciones mentales más importantes en humanos, animales y sistemas artificiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El aprendizaje automático\n",
    "\n",
    "El aprendizaje automático o Machine Learning es una rama de la inteligencia artificial que se ocupa del estudio y desarrollo de algoritmos y modelos estadísticos que permiten a las computadoras aprender a realizar una tarea y mejorar automáticamente a partir de datos, sin ser explícitamente programadas para realizar dicha tarea. \n",
    "\n",
    "La idea es que, en lugar de desarrollar un conjunto de reglas que guíen el comportamiento de la aplicación, esta se entrene con datos para que, a partir de dicho entrenamiento, pueda generalizar y realizar predicciones sobre nuevos datos, lo cual influirá en su comportamiento. Otra forma de plantearlo es que buscamos una función que nos permita predecir la variable dependiente de salida en función de una o varias variables independientes de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificación de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aprendizaje supervisado\n",
    "\n",
    "En el aprendizaje supervisado, los algoritmos se alimentan de un conjunto de datos de entrada, los cuales pueden ser continuos o discretos, junto con una variable de respuesta correspondiente. Este enfoque se divide en dos categorías principales: clasificación y regresión.\n",
    "\n",
    "- En la clasificación, el objetivo es asignar cada instancia de entrada a una categoría o clase predefinida. Por ejemplo, se puede utilizar el aprendizaje supervisado para clasificar correos electrónicos en spam o no spam, o para identificar el contenido de imágenes en diferentes categorías.\n",
    "\n",
    "- En la regresión, se aplica cuando se busca predecir un valor numérico continuo. Por ejemplo, se puede utilizar el aprendizaje supervisado para predecir el precio de una vivienda en función de características como el tamaño, la ubicación, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aprendizaje no supervisado\n",
    "\n",
    "En el aprendizaje no supervisado, los algoritmos de aprendizaje se utilizan cuando no se dispone de una variable de respuesta específica. En su lugar, el enfoque se centra en descubrir patrones, estructuras, relaciones, tendencias, agrupamientos y/o anomalías en los datos de entrada. Por lo tanto, no requiere una etiqueta o variable de respuesta predefinida, sino que se basa en la estructura inherente de los datos para realizar análisis y extraer conocimiento. Algunas técnicas comunes utilizadas en el aprendizaje no supervisado incluyen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Agrupamiento (clustering): Los algoritmos de agrupamiento buscan identificar grupos o clústeres en los datos, donde las instancias dentro de un mismo grupo son más similares entre sí que con las instancias de otros grupos. Esto puede ser útil para descubrir patrones y segmentar datos en categorías no conocidas previamente.\n",
    "\n",
    "- Análisis de componentes principales (PCA): PCA es una técnica de reducción de dimensionalidad que busca encontrar combinaciones lineales de variables para representar los datos de manera más compacta. Esto puede ayudar a identificar las principales características o dimensiones subyacentes en los datos.\n",
    "\n",
    "- Reglas de asociación: Este enfoque busca descubrir relaciones y patrones frecuentes entre diferentes variables o atributos en los datos. Por ejemplo, se puede utilizar para identificar productos que tienden a ser comprados juntos en un supermercado.\n",
    "\n",
    "- Detección de anomalías: Se utilizan algoritmos para identificar instancias o patrones inusuales o atípicos en los datos. Esto puede ser útil para detectar fraudes, errores o comportamientos anómalos en una variedad de campos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aprendizaje por refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo es una modalidad de aprendizaje automático en la cual un agente aprende a tomar decisiones en un entorno a través de recompensas o castigos por sus acciones. A diferencia de proporcionar al agente una variable de respuesta específica o etiquetas predefinidas, el agente recibe una señal de recompensa que indica qué tan bien está desempeñándose en su tarea. El objetivo del agente es aprender una política que maximice la recompensa acumulada a lo largo del tiempo. Para lograrlo, el agente debe explorar su entorno y probar diferentes acciones para descubrir cuáles resultan en mayores recompensas. Con el tiempo, el agente aprende a tomar decisiones óptimas basándose en su experiencia previa.\n",
    "\n",
    "El aprendizaje por refuerzo se aplica en diversos campos, como juegos, robótica, finanzas y control de sistemas. Algunos ejemplos de aplicaciones incluyen enseñar a un agente a jugar juegos de Atari, controlar un brazo robótico para realizar tareas específicas u optimizar una cartera de inversiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunas de las técnicas comunes utilizadas en el aprendizaje por refuerzo son:\n",
    "\n",
    "- Algoritmo Q-Learning: Un algoritmo de aprendizaje por refuerzo que utiliza una tabla (Q-table) para almacenar y actualizar los valores de recompensa esperados para pares de estados y acciones. Permite al agente tomar decisiones óptimas basadas en la exploración y explotación del entorno.\n",
    "\n",
    "- Aproximación de funciones de valor: En lugar de utilizar una tabla para almacenar valores de recompensa, se utiliza una función de valor aproximada para estimar la recompensa esperada para diferentes estados y acciones. Esto permite lidiar con espacios de estados continuos o de alta dimensionalidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Algoritmos basados en políticas: En lugar de aprender valores de recompensa esperados, estos algoritmos aprenden directamente una política que mapea estados a acciones. Pueden ser útiles cuando la exploración es costosa o cuando se necesita un comportamiento determinista.\n",
    "  \n",
    "- Métodos Monte Carlo: Estos métodos se basan en la estimación de recompensas esperadas a partir de muestras de episodios completos. Utilizan el promedio de las recompensas obtenidas en cada episodio para actualizar la política y mejorar el desempeño del agente.\n",
    "\n",
    "- Algoritmos Actor-Critic: Combina elementos de métodos basados en políticas y aproximación de funciones de valor. El actor aprende una política y el crítico evalúa la calidad de las acciones tomadas por el actor. Estos dos componentes se retroalimentan y mejoran mutuamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Desarrollo Tradicional vs Machine Learning\n",
    " \n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://www.avenga.com/wp-content/uploads/2021/12/image4-1.png\" alt=\"Mcpits\"  >\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En un desarrollo tradicional, el programador escribe un código que toma un conjunto de datos de entrada, son procesados a travez de algoritmos claramente definidos  y produce un resultado. En el aprendizaje automático, el programador escribe un algoritmo que toma un conjunto de datos de entrada y un conjunto de resultados de salida, despues de un proceso de entrenamiento el algoritmo aprende a resolver la tarea y es integrado como parte de un programa. El programa generado se puede utilizar para predecir el resultado de nuevos datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelos de Machine Learning\n",
    "\n",
    "Un modelo es una representación matemática o computacional que captura las relaciones y patrones subyacentes en los datos. Se crea a partir de un conjunto de datos de entrenamiento y se utiliza para realizar predicciones o tomar decisiones sobre nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pasos para crear un modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recopilación de datos\n",
    "\n",
    "La recopilación de datos en general es una parte esencial para el análisis y la resolución de problemas, consiste en obtener información relevante en diversas formas (texto, datos tabulares, imágenes o sonidos). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explorar y preparar los datos\n",
    "\n",
    "La preparación de datos es el proceso de transformar los datos en un formato adecuado para su posterior análisis. Esta etapa implica revisar y limpiar los datos para asegurar su calidad y coherencia, así como adaptarlos al formato requerido por el algoritmo de aprendizaje automático. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Seleccionar y entrenar un modelo\n",
    "\n",
    "Cuando los datos están listos, se elige un algoritmo de aprendizaje automático apropiado para el tipo de problema que se desea resolver. El algoritmo se entrena utilizando los datos de entrenamiento. El algoritmo aprende a resolver el problema y se ajusta a medida que se expone a más datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluación del rendimiento del modelo\n",
    "\n",
    "Cada modelo de aprendizaje automático puede generar una solución sesgada para el problema de aprendizaje, por lo que es importante evaluar qué tan bien el algoritmo ha aprendido de su experiencia. Dependiendo del tipo de modelo utilizado, es posible evaluar la precisión del modelo utilizando un conjunto de datos de prueba, o es posible que se necesite desarrollar medidas de rendimiento específicas para la aplicación prevista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Mejora del rendimiento del modelo \n",
    "\n",
    "si se necesita un mejor rendimiento, es necesario utilizar estrategias más avanzadas para aumentar el rendimiento del modelo. A veces, puede ser necesario cambiar a un tipo diferente de modelo. Es posible que necesites complementar tus datos con más recopilación de datos, o realizar trabajos preparatorios adicionales como en el paso dos de este proceso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmos Paramétricos y No Paramétricos\n",
    " \n",
    "Los algoritmos de aprendizaje automático paramétricos son aquellos que simplifican la función de mapeo a una forma conocida, lo que implica seleccionar una forma para la función y luego aprender los coeficientes correspondientes a partir de los datos de entrenamiento. Estas simplificaciones se basan en suposiciones predefinidas y permiten un proceso de aprendizaje más eficiente. Ejemplos de algoritmos paramétricos son la Regresión Lineal y la Regresión Logística.\n",
    "\n",
    "Los algoritmos de aprendizaje automático no paramétricos no hacen suposiciones fuertes sobre la forma de la función de mapeo y, por lo tanto, tienen la flexibilidad de aprender cualquier forma funcional a partir de los datos de entrenamiento. No están limitados por suposiciones específicas y, en consecuencia, pueden lograr una mayor precisión. Sin embargo, debido a su enfoque más flexible y menos restringido, los algoritmos no paramétricos suelen requerir más datos y tiempo de entrenamiento. Ejemplos de algoritmos no paramétricos son las Máquinas de Vectores de Soporte (SVM), las Redes Neuronales y los Árboles de Decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sesgo y Varianza\n",
    " \n",
    "El sesgo se refiere a las suposiciones simplificadoras que realiza un modelo para facilitar el aprendizaje de la función objetivo. Los algoritmos paramétricos tienden a tener un alto sesgo, lo que significa que hacen suposiciones fuertes sobre la forma de la función objetivo. Esto les permite aprender rápidamente y ser más fáciles de entender, pero también los hace menos flexibles en situaciones en las que las suposiciones simplificadoras no se cumplen.\n",
    "\n",
    "Los árboles de decisión son un ejemplo de algoritmo con bajo sesgo, mientras que la regresión lineal es un ejemplo de algoritmo con alto sesgo.\n",
    "\n",
    "La varianza se refiere a la cantidad en la que cambiará la estimación de la función objetivo si se utilizan diferentes conjuntos de datos de entrenamiento. Los algoritmos con alta varianza tienen una mayor sensibilidad a los datos de entrenamiento y pueden sobreajustarse a ellos, lo que puede resultar en un rendimiento deficiente en nuevos datos. Los algoritmos no paramétricos tienden a tener alta varianza, ya que no hacen suposiciones fuertes y pueden adaptarse de manera más flexible a los datos.\n",
    "\n",
    "El algoritmo de k-Vecinos más Cercanos es un ejemplo de algoritmo con alta varianza, mientras que el Análisis Discriminante Lineal es un ejemplo de algoritmo con baja varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de Regresión Lineal\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://aprendeia.com/wp-content/uploads/2018/11/48051791826_299fb750ea_o-1536x897.png&f=1&nofb=1&ipt=8fab42eac130a3884885dca954f72e76f98cbe89f7fa7b504a64d4063d2febf8&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es una técnica estadística que busca minimizar el error de un modelo predictivo.\n",
    "- Se basa en una ecuación que describe una línea que se ajusta a la relación entre variables de entrada (x) y salida (y).\n",
    "- Se pueden usar diferentes métodos para encontrar los coeficientes de la ecuación, como mínimos cuadrados ordinarios o descenso de gradiente.\n",
    "- Es una técnica antigua, rápida y simple, pero requiere eliminar variables correlacionadas y ruido de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_approximated = np.dot(X, self.weights) + self.bias\n",
    "        return y_approximated\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de Regresión Logística\n",
    " \n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://www.codificandobits.com/img/posts/2018-08-06/concepto_general_regresion.png&f=1&nofb=1&ipt=c739db721352bc884a7c88128b2ebbe7bc355d6aad20a8a6342a2d9f02d8d934&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es una técnica estadística utilizada para problemas de clasificación binaria.\n",
    "- Busca encontrar los valores para los coeficientes que ponderan cada variable de entrada.\n",
    "- La predicción se transforma mediante una función no lineal llamada función logística.\n",
    "- Las predicciones también pueden interpretarse como probabilidades de pertenencia a una clase específica.\n",
    "- Requiere eliminar atributos no relacionados con la variable de salida y atributos correlacionados entre sí.\n",
    "- Es un modelo rápido y efectivo en problemas de clasificación binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # approximate y with linear combination of weights and x, plus bias\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            # apply sigmoid function\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de Análisis Discriminante Lineal\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://lh6.googleusercontent.com/ar5TGG5URcdr3EWHMPDIzovIZp_v0KzrXHraV06tMQEHBCdyrncGshMkTD438QRNmnesmunAvuzzp1xvTEjhqLkm9XR-wIOviiOpqsplTirUZBmx9Ymm1SOuM60K1-mszWaOpvIy&f=1&nofb=1&ipt=afe485c96a4f050921b935e44c4f477ef224fbe58f9f4161f1b1da2d14733c02&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es una técnica estadística utilizada para problemas de clasificación con más de dos clases.\n",
    "- Se basa en las propiedades estadísticas de los datos calculadas para cada clase, como el valor medio y la varianza.\n",
    "- Las predicciones se realizan calculando un valor discriminante para cada clase y eligiendo la clase con el valor más alto.\n",
    "- Asume que los datos tienen una distribución gaussiana, por lo que se debe eliminar los valores atípicos antes del análisis.\n",
    "- Es un método simple y poderoso en el modelado predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python \n",
    "class LDA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.linear_discriminants = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = np.unique(y)\n",
    "\n",
    "        # Within class scatter matrix:\n",
    "        # SW = sum((X_c - mean_X_c)^2 )\n",
    "\n",
    "        # Between class scatter:\n",
    "        # SB = sum( n_c * (mean_X_c - mean_overall)^2 )\n",
    "\n",
    "        mean_overall = np.mean(X, axis=0)\n",
    "        SW = np.zeros((n_features, n_features))\n",
    "        SB = np.zeros((n_features, n_features))\n",
    "        for c in class_labels:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            # (4, n_c) * (n_c, 4) = (4,4) -> transpose\n",
    "            SW += (X_c - mean_c).T.dot((X_c - mean_c))\n",
    "\n",
    "            # (4, 1) * (1, 4) = (4,4) -> reshape\n",
    "            n_c = X_c.shape[0]\n",
    "            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\n",
    "            SB += n_c * (mean_diff).dot(mean_diff.T)\n",
    "\n",
    "        # Determine SW^-1 * SB\n",
    "        A = np.linalg.inv(SW).dot(SB)\n",
    "        # Get eigenvalues and eigenvectors of SW^-1 * SB\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "        # -> eigenvector v = [:,i] column vector, transpose for easier calculations\n",
    "        # sort eigenvalues high to low\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(abs(eigenvalues))[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        # store first n eigenvectors\n",
    "        self.linear_discriminants = eigenvectors[0 : self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # project data\n",
    "        return np.dot(X, self.linear_discriminants.T)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Análisis de Componentes Principales (PCA)\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://kindsonthegenius.com/blog/wp-content/uploads/2018/11/Principal-2BComponents-2BAnalysis-2BTutorial.jpg\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es una técnica estadística utilizada en el campo del análisis multivariante y el aprendizaje automático.\n",
    "    \n",
    "- Se utiliza para reducir la cantidad de variables en un conjunto de datos, lo que facilita la visualización y el análisis de datos complejos.\n",
    "\n",
    "- Busca los ejes (componentes principales) a lo largo de los cuales los datos tienen la mayor variabilidad. De esta manera, se conserva la mayor cantidad posible de información del conjunto de datos original.\n",
    "\n",
    "- Los componentes principales son mutuamente ortogonales, lo que significa que están no correlacionados entre sí. Esta propiedad es útil en aplicaciones donde se necesita independencia entre las variables.\n",
    "\n",
    "- Se utiliza en problemas de aprendizaje automático para preprocesar datos antes de aplicar algoritmos de modelado. También es útil en aplicaciones como reconocimiento de patrones y visión por computadora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Mean centering\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X - self.mean\n",
    "\n",
    "        # covariance, function needs samples as columns\n",
    "        cov = np.cov(X.T)\n",
    "\n",
    "        # eigenvalues, eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "        # -> eigenvector v = [:,i] column vector, transpose for easier calculations\n",
    "        # sort eigenvectors\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "\n",
    "        # store first n eigenvectors\n",
    "        self.components = eigenvectors[0 : self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # project data\n",
    "        X = X - self.mean\n",
    "        return np.dot(X, self.components.T)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Árboles de Clasificación y Regresión\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://tse4.mm.bing.net/th%3Fid=OIP.gcbLOrYcXbjtSQJZznFg7wHaFP&pid=Api&f=1&ipt=8874dd122c53feef98057618c354332770d821e77cd3263faa32d7b05a4003bb&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Son un tipo importante de algoritmo en el aprendizaje automático para el modelado predictivo.\n",
    "- La representación del modelo es un árbol binario, donde cada nodo representa una variable de entrada y un punto de división.\n",
    "- Las hojas del árbol contienen una variable de salida que se utiliza para realizar predicciones.\n",
    "- Son rápidos de aprender y muy rápidos para realizar predicciones, y no requieren una preparación especial de los datos.\n",
    "- Tienen una alta varianza y pueden generar predicciones más precisas cuando se utilizan en un conjunto (ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python \n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (\n",
    "            depth >= self.max_depth\n",
    "            or n_labels == 1\n",
    "            or n_samples < self.min_samples_split\n",
    "        ):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to information gain\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # parent loss\n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # generate split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### K-Means\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es un algoritmo de agrupamiento (clustering) utilizado en el campo de la minería de datos y el aprendizaje automático. \n",
    "- Su objetivo principal es agrupar un conjunto de datos en K grupos distintos basándose en sus características.\n",
    "- Calcula los centroides de los grupos y asigna puntos de datos al grupo cuyo centroide está más cercano en términos de distancia euclidiana.\n",
    "- Se utiliza en diversas aplicaciones, como segmentación de clientes, compresión de imágenes, análisis de redes sociales y procesamiento de imágenes y señales. También se utiliza como paso previo en algoritmos más complejos de aprendizaje automático para reducir la dimensionalidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters\n",
    "        self.plot_steps = plot_steps\n",
    "\n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "        # the centers (mean feature vector) for each cluster\n",
    "        self.centroids = []\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # initialize\n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "\n",
    "        # Optimize clusters\n",
    "        for _ in range(self.max_iters):\n",
    "            # Assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "            # Calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "\n",
    "            # check if clusters have changed\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "        # Classify samples as the index of their clusters\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "\n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        # each sample will get the label of the cluster it was assigned to\n",
    "        labels = np.empty(self.n_samples)\n",
    "\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_index in cluster:\n",
    "                labels[sample_index] = cluster_idx\n",
    "        return labels\n",
    "\n",
    "    def _create_clusters(self, centroids):\n",
    "        # Assign the samples to the closest centroids to create clusters\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        # distance of the current sample to each centroid\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_index = np.argmin(distances)\n",
    "        return closest_index\n",
    "\n",
    "    def _get_centroids(self, clusters):\n",
    "        # assign mean value of clusters to centroids\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        # distances between each old and new centroids, fol all centroids\n",
    "        distances = [\n",
    "            euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)\n",
    "        ]\n",
    "        return sum(distances) == 0\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de los k-Vecinos más Cercanos\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://tse3.mm.bing.net/th%3Fid=OIP.RZ-TalsCTOmg1vmDZIlOPwHaGZ&pid=Api&f=1&ipt=7fc62bc4d39c3127b63f067a645df82b16356958c7910e06a43fbf76738565c2&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- El modelo se compone de todo el conjunto de datos de entrenamiento.\n",
    "- Las predicciones se realizan buscando las K instancias más similares en el conjunto de entrenamiento y resumiendo la variable de salida para esas K instancias.\n",
    "- La similitud se puede medir con la distancia euclidiana si los atributos están en la misma escala.\n",
    "- Requiere mucho espacio de memoria para almacenar todos los datos, pero solo realiza cálculos cuando se necesita una predicción, lo que permite un aprendizaje en tiempo real.\n",
    "- Puede perder efectividad en dimensiones muy altas, lo que se conoce como la maldición de la dimensionalidad. Es importante seleccionar las variables de entrada más relevantes para predecir la variable de salida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_idx = np.argsort(distances)[: self.k]\n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_neighbor_labels = [self.y_train[i] for i in k_idx]\n",
    "        # return the most common class label\n",
    "        most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Perceptrón\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/400px-Perceptr%C3%B3n_5_unidades.svg.png\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_func = self._unit_step_func\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "\n",
    "            for idx, x_i in enumerate(X):\n",
    "\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activation_func(linear_output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.lr * (y_[idx] - y_predicted)\n",
    "\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.activation_func(linear_output)\n",
    "        return y_predicted\n",
    "\n",
    "    def _unit_step_func(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es un algoritmo de aprendizaje supervisado en el campo del aprendizaje automático y la inteligencia artificial\n",
    "-  Es una unidad básica de una red neuronal y se utiliza para clasificar objetos en dos categorías, lo que lo convierte en un clasificador binario.\n",
    "-  El perceptrón toma múltiples entradas, las multiplica por pesos asociados y las suma. Esta suma ponderada se pasa a través de una función de activación para determinar la salida del perceptrón.\n",
    "-  La función de activación generalmente utiliza un umbral (también llamado \"bias\") para decidir si activar la salida del perceptrón. Si la suma ponderada de las entradas supera el umbral, el perceptrón emite una salida (a menudo 1); de lo contrario, emite 0.\n",
    "-  El perceptrón ajusta sus pesos durante el proceso de entrenamiento para minimizar los errores de clasificación en los datos de entrenamiento. Esto se hace utilizando un algoritmo de aprendizaje supervisado que actualiza los pesos cuando el perceptrón clasifica incorrectamente un ejemplo.\n",
    "-  Los perceptrones tienen limitaciones y solo pueden aprender patrones linealmente separables, lo que significa que solo pueden separar clases con una única línea recta en el espacio de entrada. Para superar esta limitación, se utilizan redes neuronales multicapa (también conocidas como perceptrones multicapa) que incorporan capas ocultas y funciones de activación no lineales para aprender patrones más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vector de Cuantización de Aprendizaje (Learning Vector Quantization, LVQ)\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-03-08-005747.jpg\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es un algoritmo de redes neuronales artificiales que permite reducir los requisitos de memoria del conjunto de datos de entrenamiento.\n",
    "- El modelo se compone de una colección de vectores de código que se seleccionan al azar y se adaptan para resumir mejor el conjunto de datos de entrenamiento.\n",
    "- Las predicciones se realizan encontrando el vecino más similar (el vector de código que mejor coincide) calculando la distancia entre cada vector de código y la nueva instancia de datos.\n",
    "- Se obtienen mejores resultados si los datos se reescalan para tener el mismo rango, como entre 0 y 1.\n",
    "- Es una buena opción si KNN ofrece buenos resultados en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python  \n",
    "class LVQ:\n",
    "    def __init__(self, n_classes, n_neurons, lr=0.1):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_neurons = n_neurons\n",
    "        self.lr = lr\n",
    "        self.weights = np.random.rand(n_neurons, n_classes)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        for i in range(inputs.shape[0]):\n",
    "            input_vector = inputs[i]\n",
    "            target_vector = targets[i]\n",
    "            winner_index = self._find_winner(input_vector)\n",
    "            self._update_weights(winner_index, input_vector, target_vector)\n",
    "\n",
    "    def _find_winner(self, input_vector):\n",
    "        distances = np.linalg.norm(self.weights - input_vector, axis=1)\n",
    "        return np.argmin(distances)\n",
    "\n",
    "    def _update_weights(self, winner_index, input_vector, target_vector):\n",
    "        winner_weight = self.weights[winner_index]\n",
    "        if np.array_equal(winner_weight, target_vector):\n",
    "            self.weights[winner_index] += self.lr * (input_vector - winner_weight)\n",
    "        else:\n",
    "            self.weights[winner_index] -= self.lr * (input_vector - winner_weight)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        winners = []\n",
    "        for i in range(inputs.shape[0]):\n",
    "            input_vector = inputs[i]\n",
    "            winner_index = self._find_winner(input_vector)\n",
    "            winners.append(winner_index)\n",
    "        return np.array(winners)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Máquinas de Soporte Vectorial (Support Vector Machines, SVM)\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://dataaspirant.com/wp-content/uploads/2020/12/3-Support-Vector-Machine-Algorithm.png&f=1&nofb=1&ipt=8751ff06c192c90ac12bbbe558a10d42de14e6146f8f90b7d79a981b534c708f&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (\n",
    "                        2 * self.lambda_param * self.w - np.dot(x_i, y_[idx])\n",
    "                    )\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es un algoritmo de aprendizaje supervisado para problemas de clasificación y regresión.\n",
    "- El modelo se compone de un hiperplano que separa el espacio de variables de entrada por clase.\n",
    "- El objetivo es encontrar el hiperplano que mejor separa los puntos de datos de las diferentes clases, maximizando el margen entre ellos.\n",
    "- Solo los puntos de datos más cercanos al hiperplano, llamados vectores de soporte, son relevantes para definir el hiperplano y construir el clasificador.\n",
    "- Se utiliza un algoritmo de optimización para encontrar los valores de los coeficientes que definen el hiperplano.\n",
    "- Es uno de los clasificadores más efectivos disponibles y vale la pena probarlo en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bagging y Bosques Aleatorios\n",
    " \n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://editor.analyticsvidhya.com/uploads/6690812.png&f=1&nofb=1&ipt=a41bf3f764a67752a839058a4568bbeffc719b7d2d4b7c1e5e2075cec1bca700&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    " \n",
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "\n",
    "def most_common_label(y):\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_depth=self.max_depth,\n",
    "                n_feats=self.n_feats,\n",
    "            )\n",
    "            X_samp, y_samp = bootstrap_sample(X, y)\n",
    "            tree.fit(X_samp, y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
    "        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Son un tipo de algoritmo de aprendizaje en conjunto que utiliza el método de bootstrap para estimar modelos estadísticos completos, como los árboles de decisión.\n",
    "- Se toman múltiples muestras de los datos de entrenamiento y se construyen modelos para cada muestra.\n",
    "- Las predicciones de todos los modelos se promedian para obtener una mejor estimación del valor de salida.\n",
    "- Los Bosques Aleatorios son una variante del bagging en la que se introducen divisiones subóptimas mediante la introducción de aleatoriedad en los árboles de decisión.\n",
    "- Esto permite que los modelos en el bosque sean más diferentes entre sí, pero siguen siendo precisos de diferentes maneras.\n",
    "- Son útiles para mejorar el rendimiento de algoritmos con alta varianza, como los árboles de decisión, ya que ayudan a reducir la varianza y mejorar la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Boosting y AdaBoost\n",
    " \n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https://miro.medium.com/max/1200/1*tLUhrb27BKMtXAXRfy15Vw.png&f=1&nofb=1&ipt=6185d4350cc6345b56657d0b87c12b8aa55e4fd60add34e072c0d72daf0d5a47&ipo=images\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# Decision stump used as weak classifier\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[X_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X_column > self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Adaboost:\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.clfs = []\n",
    "\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            min_error = float(\"inf\")\n",
    "\n",
    "            # greedy search to find best threshold and feature\n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[:, feature_i]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = w[y != predictions]\n",
    "                    error = sum(misclassified)\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Es una técnica de aprendizaje en conjunto que busca crear un clasificador fuerte a partir de varios clasificadores débiles.\n",
    "\n",
    "- Se construye un modelo inicial y luego se crean modelos adicionales que intentan corregir los errores del modelo anterior.\n",
    "\n",
    "- AdaBoost es uno de los primeros y más exitosos algoritmos de Boosting para la clasificación binaria.\n",
    "\n",
    "- Se utiliza con árboles de decisión cortos y asigna más peso a los datos de entrenamiento difíciles de predecir y menos peso a los datos fáciles de predecir.\n",
    "\n",
    "- Los modelos se crean secuencialmente, uno después del otro, y se actualizan los pesos en las instancias de entrenamiento para mejorar el aprendizaje del siguiente modelo.\n",
    "\n",
    "- Las predicciones para nuevos datos se ponderan según la precisión de cada modelo en los datos de entrenamiento.\n",
    "\n",
    "- Es importante tener datos limpios y sin valores atípicos, ya que el algoritmo se centra en corregir errores y necesita datos de calidad para obtener buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "<div class=\"imagen\">\n",
    "<img src=\"https://insightimi.files.wordpress.com/2020/04/unnamed-1.png\" alt=\"Mcpits\"  >\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Naive Bayes asume que las características son independientes entre sí dado el valor de la clase. Esta es la razón por la que se le llama \"naive\" (ingenuo). Aunque esta suposición rara vez es cierta en aplicaciones reales, el algoritmo puede funcionar sorprendentemente bien en muchas situaciones.\n",
    "\n",
    "-  Naive Bayes se basa en el teorema de Bayes, que establece cómo se pueden actualizar las probabilidades a priori de las hipótesis cuando se observan nuevas evidencias. En el contexto del aprendizaje automático, se utiliza para calcular la probabilidad de que un objeto pertenezca a una clase particular dadas sus características.\n",
    "\n",
    "- Existen diferentes variantes de Naive Bayes, como el Naive Bayes Gaussiano (para características continuas), el Naive Bayes Multinomial (para datos discretos como conteos de palabras) y el Naive Bayes Bernoulli (para datos binarios). La elección del modelo adecuado depende del tipo de datos con el que estés trabajando.\n",
    "\n",
    "- Naive Bayes es rápido y eficiente, lo que lo hace adecuado para conjuntos de datos grandes. Además, requiere una cantidad relativamente pequeña de datos para estimar los parámetros del modelo, lo que lo convierte en una opción atractiva cuando tienes pocos datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metricas y Funciones de Pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regresión\n",
    "\n",
    "Mean Squared Error (MSE): Representa el promedio de los cuadrados de las diferencias entre las predicciones y los valores reales. Cuanto menor sea el MSE, mejor será el modelo.\n",
    "\n",
    "Mean Absolute Error (MAE): Calcula el promedio de las diferencias absolutas entre las predicciones y los valores reales. Es menos sensible a los valores extremos en comparación con MSE.\n",
    "\n",
    "Mean Squared Logarithmic Error (MSLE): Similar a MSE, pero aplica el logaritmo a las predicciones y los valores reales antes de calcular el error cuadrático medio. Útil cuando las magnitudes de los valores varían ampliamente.\n",
    "\n",
    "Explained Variance Score: Mide la proporción de la varianza en los datos que es explicada por el modelo. El valor oscila entre 0 y 1, donde 1 indica una predicción perfecta.\n",
    "\n",
    "R² Score (Coefficient of Determination): Indica qué porcentaje de la variabilidad en los datos es explicado por el modelo. También oscila entre 0 y 1, y un valor más cercano a 1 es mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Clasificación binaria \n",
    "\n",
    "Log Loss (Logarithmic Loss): Mide el rendimiento de un clasificador donde las predicciones son probabilidades entre 0 y 1. Cuanto menor sea el log loss, mejor será el clasificador.\n",
    "\n",
    "Hinge Loss: Se utiliza en máquinas de vectores de soporte (SVM) y es una métrica de pérdida que se minimiza durante el entrenamiento del modelo. No es una métrica de evaluación en sí misma.\n",
    "\n",
    "Brier Score Loss: Mide la diferencia entre las probabilidades predichas y las observadas. Cuanto menor sea el Brier Score, mejor será el clasificador.\n",
    "\n",
    "ROC AUC Score: Área bajo la curva ROC (Receiver Operating Characteristic). Mide la capacidad del modelo para distinguir entre clases positivas y negativas. Un valor de 1 indica un modelo perfecto.\n",
    "\n",
    "Average Precision Score: Calcula la precisión promedio para diferentes valores umbral de probabilidad en problemas de clasificación binaria.\n",
    "\n",
    "Precision-Recall Curve: Aunque no es una métrica única, muestra la relación entre la precisión y el recall para diferentes umbrales de probabilidad y es útil para evaluar modelos en problemas de desequilibrio de clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clasificación multiclase \n",
    "\n",
    "Log Loss: La explicación es la misma que para clasificación binaria.\n",
    "\n",
    "Hinge Loss: Similar a la clasificación binaria, se usa en máquinas de vectores de soporte para problemas de clasificación multiclase.\n",
    "\n",
    "Jaccard Score: Calcula la similitud de Jaccard entre conjuntos, es decir, la intersección dividida por la unión de los conjuntos. Es útil para problemas de clasificación multiclase con conjuntos de etiquetas.\n",
    "\n",
    "F1 Score: La media armónica de precisión y recall. Es útil cuando hay desequilibrio de clases.\n",
    "\n",
    "Precision Score y Recall Score: Son métricas individuales que también se aplican a la clasificación multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Usos del Machine Learning\n",
    "    \n",
    "- Sistemas de recomendación y marketing personalizado\n",
    "- Segmentación de mercado\n",
    "- Chatbots\n",
    "- Navegación autónoma\n",
    "- Mantenimiento predictivo\n",
    "- Generación de contenido\n",
    "- Clasificación de datos\n",
    "- Predicción y pronóstico\n",
    "- Reconocimiento de voz y procesamiento del lenguaje natural\n",
    "- Visión por computadora\n",
    "- Medicina y salud\n",
    "- Automatización industrial\n",
    "- Seguridad cibernética"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
